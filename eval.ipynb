{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span  style=\"font-size: 20px; line-height: 30px;\">\n",
    "Calculate metrics:\n",
    "    \n",
    "<ol>\n",
    "    <li> F1 </li>\n",
    "    <li> Mean IoU </li>\n",
    "    <li> Recall </li>\n",
    "    <li> Precision </li>\n",
    "    <li> Accuracy </li>\n",
    "</ol>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load predict mask and ground truth Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask = sorted(glob(os.path.join(\"prediction\", \"*\")))\n",
    "true_mask = sorted(glob(os.path.join(\"dataset\", \"test\", \"masks\", \"*\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                | 7/306 [00:01<01:14,  4.03it/s]C:\\Users\\HuyTrng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 31%|█████████████████████████▍                                                       | 96/306 [00:23<00:52,  4.04it/s]C:\\Users\\HuyTrng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 44%|███████████████████████████████████▌                                            | 136/306 [00:33<00:42,  4.04it/s]C:\\Users\\HuyTrng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 60%|████████████████████████████████████████████████                                | 184/306 [00:45<00:30,  4.00it/s]C:\\Users\\HuyTrng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 306/306 [01:16<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "\n",
    "for pred_y, true_y in tqdm(zip(pred_mask, true_mask), total=len(pred_mask)):\n",
    "    name = pred_y.split(\"/\")[-1]\n",
    "    \n",
    "    pred_y = cv2.imread(pred_y, cv2.IMREAD_GRAYSCALE)\n",
    "    pred_y = pred_y/255.0\n",
    "    pred_y = pred_y > 0.5\n",
    "    pred_y = pred_y.astype(np.int32)\n",
    "    pred_y = pred_y.flatten()\n",
    "    \n",
    "    true_y = cv2.imread(true_y, cv2.IMREAD_GRAYSCALE)\n",
    "    true_y = true_y/255.0\n",
    "    true_y = true_y > 0.5\n",
    "    true_y = true_y.astype(np.int32)\n",
    "    true_y = true_y.flatten()\n",
    "    \n",
    "    acc_value = accuracy_score(pred_y, true_y)\n",
    "    f1_value = f1_score(pred_y, true_y, labels=[0, 1], average=\"binary\")\n",
    "    jac_value = jaccard_score(pred_y, true_y, labels=[0, 1], average=\"binary\")\n",
    "    recall_value = recall_score(pred_y, true_y, labels=[0, 1], average=\"binary\")\n",
    "    precision_value = precision_score(pred_y, true_y, labels=[0, 1], average=\"binary\")\n",
    "    score.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99427\n",
      "F1: 0.79447\n",
      "Jaccard: 0.70679\n",
      "Recall: 0.85322\n",
      "Precision: 0.78579\n"
     ]
    }
   ],
   "source": [
    "score = [s[1:]for s in score]\n",
    "score = np.mean(score, axis=0)\n",
    "print(f\"Accuracy: {score[0]:0.5f}\")\n",
    "print(f\"F1: {score[1]:0.5f}\")\n",
    "print(f\"Jaccard: {score[2]:0.5f}\")\n",
    "print(f\"Recall: {score[3]:0.5f}\")\n",
    "print(f\"Precision: {score[4]:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
